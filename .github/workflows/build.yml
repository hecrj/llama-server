name: Build
on:
  workflow_dispatch: {}
  push:
    branches:
      - master

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref && github.ref || github.run_id }}
  cancel-in-progress: true

jobs:
  find:
    runs-on: ubuntu-latest
    outputs:
      build: ${{ steps.get_latest.outputs.tag }}
    steps:
      - name: Get latest build
        id: get_latest
        run: |
          build=$(curl -s https://api.github.com/repos/ggml-org/llama.cpp/releases/latest | jq -r .tag_name)
          echo "llama.cpp latest build: $build"
          echo "tag=$build" >> "$GITHUB_OUTPUT"

  linux-cuda:
    needs: find
    runs-on: ubuntu-latest
    steps:
      - name: Clone latest llama.cpp
        run: git clone https://github.com/ggerganov/llama.cpp --depth=1

      - name: Pull `llama-server` image
        run: docker pull ghcr.io/ggml-org/llama.cpp:server-cuda

      - name: Create llama-server container
        run: docker create --name llama-server ghcr.io/ggml-org/llama.cpp:server-cuda

      - name: Copy build artifacts from container
        run: docker cp llama-server:/app .

      - name: Copy CUDA runtime from container
        run: |
          CUDA_VERSION=$(docker run --rm --entrypoint "" ghcr.io/ggml-org/llama.cpp:server-cuda \
              bash -c "ls /usr/local | grep '^cuda-[0-9]\+\.[0-9]\+$' | sort -V | tail -n1 | sed 's/cuda-//'")

          CUDA_MAJOR_VERSION=${CUDA_VERSION%%.*}

          docker cp -L llama-server:/usr/local/cuda-$CUDA_VERSION/targets/x86_64-linux/lib/libcudart.so.$CUDA_MAJOR_VERSION app/.
          docker cp -L llama-server:/usr/local/cuda-$CUDA_VERSION/targets/x86_64-linux/lib/libcublas.so.$CUDA_MAJOR_VERSION app/.
          docker cp -L llama-server:/usr/local/cuda-$CUDA_VERSION/targets/x86_64-linux/lib/libcublasLt.so.$CUDA_MAJOR_VERSION app/.

      - name: Set $ORIGIN as RPATH for `llama-server` and shared libraries
        run: |
          patchelf --set-rpath '$ORIGIN' app/llama-server
          patchelf --set-rpath '$ORIGIN' app/*.so

      - name: Separate CUDA backend
        run: |
          mkdir cuda
          mv app/libcudart* app/libcublas* app/libggml-cuda.so cuda/.

      - name: Pack llama-server
        run: |
          cp llama.cpp/LICENSE app/.
          cd app
          zip -r ../llama-server-${{ needs.find.outputs.build }}-linux-x64.zip *

      - name: Pack CUDA backend
        run: |
          cp llama.cpp/LICENSE cuda/.
          cd cuda
          zip -r ../backend-cuda-${{ needs.find.outputs.build }}-linux-x64.zip *

      - name: Upload llama-server
        uses: actions/upload-artifact@v4
        with:
          path: llama-server-${{ needs.find.outputs.build }}-linux-x64.zip
          name: llama-server-linux-x64

      - name: Upload CUDA backend
        uses: actions/upload-artifact@v4
        with:
          path: backend-cuda-${{ needs.find.outputs.build }}-linux-x64.zip
          name: backend-cuda-linux-x64

  windows:
    needs: find
    runs-on: ubuntu-latest
    steps:
      - name: Download Windows release
        run: curl -LO https://github.com/ggml-org/llama.cpp/releases/download/${{ needs.find.outputs.build }}/llama-${{ needs.find.outputs.build }}-bin-win-cpu-x64.zip

      - name: Unzip Windows release
        run: unzip llama-${{ needs.find.outputs.build }}-bin-win-cpu-x64.zip

      - name: Pack llama-server
        run: |
          mkdir app
          mv *.dll app/.
          mv llama-server.exe app/.
          cd app
          zip -r ../llama-server-${{ needs.find.outputs.build }}-windows-x64.zip *

      - name: Upload llama-server
        uses: actions/upload-artifact@v4
        with:
          path: llama-server-${{ needs.find.outputs.build }}-windows-x64.zip
          name: llama-server-windows-x64
