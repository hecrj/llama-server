name: Build
on:
  workflow_dispatch: {}
  push:
    branches:
      - master

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref && github.ref || github.run_id }}
  cancel-in-progress: true

jobs:
  linux-cuda:
    runs-on: ubuntu-22.04
    steps:
      - name: Pull `llama-server` image
        run: docker pull ghcr.io/ggml-org/llama.cpp:server-cuda

      - name: Create llama-server container
        run: docker create --name llama-server ghcr.io/ggml-org/llama.cpp:server-cuda

      - name: Copy build artifacts from container
        run: docker cp llama-server:/app .

      - name: Copy CUDA runtime from container
        run: |
          docker cp -L llama-server:/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudart.so.12 app/.
          docker cp -L llama-server:/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcublas.so.12 app/.
          docker cp -L llama-server:/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcublasLt.so.12 app/.

      - name: Set $ORIGIN as RPATH in `llama-server`
        run: patchelf --set-rpath '$ORIGIN' app/llama-server

      - name: Pack build artifacts
        run: |
          cp llama.cpp/LICENSE app/.
          zip -r llama-server-linux-x64-cuda.zip app/*

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          path: llama-server-linux-x64-cuda.zip
          name: llama-server-linux-x64-cuda.zip
