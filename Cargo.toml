[package]
name = "llama-server"
version = "0.1.1"
edition = "2024"
description = "Download, embed, and run llama.cpp in your Rust projects"
license = "MIT"
repository = "https://github.com/hecrj/llama-server"

[dependencies]
bitflags = "2"
directories = "6"
futures = "0.3"
sipper = "0.1"
zip = "7"

reqwest.version = "0.13"
reqwest.features = ["json"]

serde.version = "1"
serde.features = ["derive"]

tokio.version = "1"
tokio.features = ["rt", "fs", "io-util", "process"]

[dev-dependencies]
tokio.version = "1"
tokio.features = ["macros"]

[lints]
workspace = true

[workspace.lints.rust]
rust_2018_idioms = { level = "deny", priority = -1 }
unsafe_code = "deny"
unused_results = "deny"
missing_docs = "deny"
